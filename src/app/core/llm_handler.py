import litellm
import httpx
import logging
from config.settings import settings


logger = logging.getLogger(__name__)
litellm._turn_on_debug()


class LLMNotAvailableError(Exception):
    """Custom exception for when the LLM service is unreachable or fails."""

    pass


async def call_llm(prompt: str, model_name: str = settings.OLLAMA_MODEL_NAME) -> str:
    """
    Sends a prompt to the configured LLM via litellm and returns the response content.

    Args:
        prompt: The complete prompt string to send to the LLM.
        model_name: The specific Ollama model name to use.

    Returns:
        The text content generated by the LLM.

    Raises:
        LLMNotAvailableError: If the LLM call fails due to connection issues or API errors.
    """
    logger.debug(
        f"Attempting LLM call to model: ollama/{model_name} at {settings.OLLAMA_API_BASE_URL}"
    )
    try:
        response = await litellm.acompletion(
            model=f"ollama/{model_name}",
            messages=[{"role": "system", "content": prompt}],
            api_base=settings.OLLAMA_API_BASE_URL,
            temperature=0.0,
            max_tokens=500,
            timeout=60,
        )
        if (
            response.choices
            and response.choices[0].message
            and response.choices[0].message.content
        ):
            generated_text = response.choices[0].message.content.strip()
            logger.debug(
                f"LLM Response received: {generated_text[:100]}..."
            )  # Log snippet
            return generated_text
        else:
            logger.error(f"LLM response structure unexpected: {response}")
            raise LLMNotAvailableError(
                "LLM returned an unexpected or empty response structure."
            )

    except httpx.ConnectError as e:
        logger.error(
            f"Connection Error: Could not connect to Ollama at {settings.OLLAMA_API_BASE_URL}. Details: {e}"
        )
        raise LLMNotAvailableError(
            f"Could not connect to Ollama at {settings.OLLAMA_API_BASE_URL}"
        ) from e
    except litellm.exceptions.Timeout as e:
        logger.error(f"LLM call timed out after {e.timeout} seconds.")
        raise LLMNotAvailableError("The request to the LLM timed out.") from e
    except litellm.exceptions.APIConnectionError as e:
        logger.error(f"LLM API Connection Error: {e}", exc_info=False)
        raise LLMNotAvailableError(f"Could not connect to the LLM API: {e}") from e
    except litellm.exceptions.APIError as e:
        logger.error(
            f"LLM API Error: Status={e.status_code}, Message={e.message}",
            exc_info=False,
        )
        raise LLMNotAvailableError(
            f"The LLM API returned an error (Status {e.status_code}): {e.message}"
        ) from e
    except Exception as e:
        logger.exception(f"An unexpected error occurred during the LLM call: {e}")
        raise LLMNotAvailableError(
            f"An unexpected error occurred while communicating with the LLM: {e}"
        ) from e
