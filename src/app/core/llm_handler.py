import litellm
import httpx
import logging
from typing import AsyncGenerator

from config.settings import settings

logger = logging.getLogger(__name__)


class LLMNotAvailableError(Exception):
    """Custom exception for when the LLM service is unreachable or fails."""

    pass


async def stream_llm_response(
    prompt: str, model_name: str = settings.OLLAMA_MODEL_NAME
) -> AsyncGenerator[str, None]:
    """
    Sends a prompt to the configured LLM via litellm and streams the response chunks.

    Args:
        prompt: The complete prompt string to send to the LLM.
        model_name: The specific Ollama model name to use.

    Yields:
        Chunks of the text content generated by the LLM.

    Raises:
        LLMNotAvailableError: If the LLM call fails due to connection issues,
                                API errors before streaming starts, or if the
                                stream itself indicates a critical error.
    """
    logger.debug(
        f"Attempting streaming LLM call to model: ollama/{model_name} "
        f"at API base: {settings.OLLAMA_API_BASE_URL}"
    )
    try:
        response_stream = await litellm.acompletion(
            model=f"ollama/{model_name}",
            messages=[{"role": "user", "content": prompt}],
            api_base=settings.OLLAMA_API_BASE_URL,
            stream=True,
            stream_options={"include_usage": True},
            timeout=settings.LLM_TIMEOUT,
        )

        async for chunk in response_stream:
            content = chunk.choices[0].delta.content
            if content:
                yield content
            # Optional: Check for specific error signals within the stream if needed
            elif chunk.get("error"):  # Example check if litellm stream includes errors
                error_message = chunk["error"].get("message", "Unknown stream error")
                logger.error(f"Error received during LLM stream: {error_message}")
                raise LLMNotAvailableError(f"LLM stream error: {error_message}")

    except httpx.ConnectError as e:
        logger.error(
            f"Connection error calling LLM at {settings.OLLAMA_API_BASE_URL}: {e}"
        )
        raise LLMNotAvailableError(
            f"Could not connect to LLM service at {settings.OLLAMA_API_BASE_URL}"
        ) from e
    except litellm.exceptions.APIConnectionError as e:
        logger.error(f"LiteLLM API connection error for model {model_name}: {e}")
        raise LLMNotAvailableError(
            f"API connection error for model {model_name}"
        ) from e
    except litellm.exceptions.Timeout as e:
        logger.error(f"LiteLLM timeout error for model {model_name}: {e}")
        raise LLMNotAvailableError(f"LLM call timed out for model {model_name}") from e
    except litellm.exceptions.APIError as e:
        logger.error(
            f"LiteLLM API error (status {e.status_code}) for model {model_name}: {e.message}"
        )
        raise LLMNotAvailableError(
            f"LLM API error (status {e.status_code}): {e.message}"
        ) from e
    except Exception as e:
        logger.exception(f"An unexpected error occurred during LLM call: {e}")
        raise LLMNotAvailableError(f"An unexpected error occurred: {e}") from e

    logger.debug("LLM stream finished.")


async def call_llm(prompt: str, model_name: str = settings.OLLAMA_MODEL_NAME) -> str:
    """
    Sends a prompt to the configured LLM via litellm and returns the complete response.

    This function aggregates the streamed response for non-streaming use cases.

    Args:
        prompt: The complete prompt string to send to the LLM.
        model_name: The specific Ollama model name to use.

    Returns:
        The complete text content generated by the LLM.

    Raises:
        LLMNotAvailableError: If the LLM call fails.
    """
    full_response = ""
    try:
        async for chunk in stream_llm_response(prompt, model_name):
            full_response += chunk
    except LLMNotAvailableError:
        # Already logged in stream_llm_response
        raise
    except Exception as e:
        # Catch any other unexpected errors during aggregation
        logger.exception(f"An unexpected error occurred aggregating LLM stream: {e}")
        raise LLMNotAvailableError(f"Failed to aggregate LLM stream: {e}") from e

    if not full_response:
        logger.warning("LLM returned an empty response after streaming.")
        # Decide if empty response is an error or valid case
        # raise LLMNotAvailableError("LLM returned an empty result.")

    logger.debug("LLM call (aggregated from stream) completed.")
    return full_response
